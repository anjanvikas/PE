{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-28T12:21:45.795072Z","iopub.execute_input":"2021-10-28T12:21:45.795395Z","iopub.status.idle":"2021-10-28T12:22:24.481895Z","shell.execute_reply.started":"2021-10-28T12:21:45.795315Z","shell.execute_reply":"2021-10-28T12:22:24.481162Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nimport shutil\nfrom typing import Tuple, List\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nfrom collections import OrderedDict, Iterable\nimport warnings\nimport os\nimport csv\nimport io\nimport torch\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\nfrom typing import Callable, List, Union\nfrom torch.utils.data import Dataset\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\nfrom skimage import io\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom torch.nn.modules.loss import _Loss as Loss\nfrom torch import nn\nimport numpy as np\nimport torch.nn.functional as F\nimport torch\nfrom typing import Dict\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom typing import Callable\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\nfrom typing import Callable, List, Union\nimport torch\nfrom collections import OrderedDict\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom typing import Dict, List, Callable, Union\nimport argparse\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.utils.data import Sampler\nfrom typing import List, Iterable, Callable, Tuple\nimport numpy as np\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:24.483762Z","iopub.execute_input":"2021-10-28T12:22:24.484007Z","iopub.status.idle":"2021-10-28T12:22:26.727934Z","shell.execute_reply.started":"2021-10-28T12:22:24.483973Z","shell.execute_reply":"2021-10-28T12:22:26.727217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config ","metadata":{}},{"cell_type":"code","source":"PATH = os.path.dirname(os.path.realpath('./'))\n\nDATA_PATH ='./'\n\nEPSILON = 1e-8\n\nif DATA_PATH is None:\n    raise Exception('Configure your data folder location in config.py before continuing!')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.729293Z","iopub.execute_input":"2021-10-28T12:22:26.729544Z","iopub.status.idle":"2021-10-28T12:22:26.734773Z","shell.execute_reply.started":"2021-10-28T12:22:26.729512Z","shell.execute_reply":"2021-10-28T12:22:26.733684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils ","metadata":{}},{"cell_type":"code","source":"def mkdir(dir):\n    \"\"\"Create a directory, ignoring exceptions\n\n    # Arguments:\n        dir: Path of directory to create\n    \"\"\"\n    try:\n        os.mkdir(dir)\n    except:\n        pass\n\n\ndef rmdir(dir):\n    \"\"\"Recursively remove a directory and contents, ignoring exceptions\n\n   # Arguments:\n       dir: Path of directory to recursively remove\n   \"\"\"\n    try:\n        shutil.rmtree(dir)\n    except:\n        pass\n\n\ndef setup_dirs():\n    \"\"\"Creates directories for this project.\"\"\"\n    mkdir(PATH + '/logs/')\n    mkdir(PATH + '/logs/proto_nets')\n    mkdir(PATH + '/logs/matching_nets')\n    mkdir(PATH + '/logs/maml')\n    mkdir(PATH + '/models/')\n    mkdir(PATH + '/models/proto_nets')\n    mkdir(PATH + '/models/matching_nets')\n    mkdir(PATH + '/models/maml')\n\n\ndef pairwise_distances(x: torch.Tensor,\n                       y: torch.Tensor,\n                       matching_fn: str) -> torch.Tensor:\n    \"\"\"Efficiently calculate pairwise distances (or other similarity scores) between\n    two sets of samples.\n\n    # Arguments\n        x: Query samples. A tensor of shape (n_x, d) where d is the embedding dimension\n        y: Class prototypes. A tensor of shape (n_y, d) where d is the embedding dimension\n        matching_fn: Distance metric/similarity score to compute between samples\n    \"\"\"\n    n_x = x.shape[0]\n    n_y = y.shape[0]\n\n    if matching_fn == 'l2':\n        distances = (\n                x.unsqueeze(1).expand(n_x, n_y, -1) -\n                y.unsqueeze(0).expand(n_x, n_y, -1)\n        ).pow(2).sum(dim=2)\n        return distances\n    elif matching_fn == 'cosine':\n        normalised_x = x / (x.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n        normalised_y = y / (y.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n\n        expanded_x = normalised_x.unsqueeze(1).expand(n_x, n_y, -1)\n        expanded_y = normalised_y.unsqueeze(0).expand(n_x, n_y, -1)\n\n        cosine_similarities = (expanded_x * expanded_y).sum(dim=2)\n        return 1 - cosine_similarities\n    elif matching_fn == 'dot':\n        expanded_x = x.unsqueeze(1).expand(n_x, n_y, -1)\n        expanded_y = y.unsqueeze(0).expand(n_x, n_y, -1)\n\n        return -(expanded_x * expanded_y).sum(dim=2)\n    else:\n        raise(ValueError('Unsupported similarity function'))\n\n\ndef copy_weights(from_model: torch.nn.Module, to_model: torch.nn.Module):\n    \"\"\"Copies the weights from one model to another model.\n\n    # Arguments:\n        from_model: Model from which to source weights\n        to_model: Model which will receive weights\n    \"\"\"\n    if not from_model.__class__ == to_model.__class__:\n        raise(ValueError(\"Models don't have the same architecture!\"))\n\n    for m_from, m_to in zip(from_model.modules(), to_model.modules()):\n        is_linear = isinstance(m_to, torch.nn.Linear)\n        is_conv = isinstance(m_to, torch.nn.Conv2d)\n        is_bn = isinstance(m_to, torch.nn.BatchNorm2d)\n        if is_linear or is_conv or is_bn:\n            m_to.weight.data = m_from.weight.data.clone()\n            if m_to.bias is not None:\n                m_to.bias.data = m_from.bias.data.clone()\n\n\ndef autograd_graph(tensor: torch.Tensor) -> Tuple[\n            List[torch.autograd.Function],\n            List[Tuple[torch.autograd.Function, torch.autograd.Function]]\n        ]:\n    \"\"\"Recursively retrieves the autograd graph for a particular tensor.\n\n    # Arguments\n        tensor: The Tensor to retrieve the autograd graph for\n\n    # Returns\n        nodes: List of torch.autograd.Functions that are the nodes of the autograd graph\n        edges: List of (Function, Function) tuples that are the edges between the nodes of the autograd graph\n    \"\"\"\n    nodes, edges = list(), list()\n\n    def _add_nodes(tensor):\n        if tensor not in nodes:\n            nodes.append(tensor)\n\n            if hasattr(tensor, 'next_functions'):\n                for f in tensor.next_functions:\n                    if f[0] is not None:\n                        edges.append((f[0], tensor))\n                        _add_nodes(f[0])\n\n            if hasattr(tensor, 'saved_tensors'):\n                for t in tensor.saved_tensors:\n                    edges.append((t, tensor))\n                    _add_nodes(t)\n\n    _add_nodes(tensor.grad_fn)\n\n    return nodes, edges","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.736902Z","iopub.execute_input":"2021-10-28T12:22:26.737287Z","iopub.status.idle":"2021-10-28T12:22:26.759905Z","shell.execute_reply.started":"2021-10-28T12:22:26.737252Z","shell.execute_reply":"2021-10-28T12:22:26.759195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for root, folders, _ in os.walk('./'):\n    for folder in folders:\n        rmdir(os.path.join(root,folder))","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.761176Z","iopub.execute_input":"2021-10-28T12:22:26.76148Z","iopub.status.idle":"2021-10-28T12:22:26.771845Z","shell.execute_reply.started":"2021-10-28T12:22:26.761393Z","shell.execute_reply":"2021-10-28T12:22:26.770964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics ","metadata":{}},{"cell_type":"code","source":"\ndef categorical_accuracy(y, y_pred):\n    \"\"\"Calculates categorical accuracy.\n    # Arguments:\n        y_pred: Prediction probabilities or logits of shape [batch_size, num_categories]\n        y: Ground truth categories. Must have shape [batch_size,]\n    \"\"\"\n    return torch.eq(y_pred.argmax(dim=-1), y).sum().item() / y_pred.shape[0]\n\n\nNAMED_METRICS = {\n    'categorical_accuracy': categorical_accuracy\n}","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.773259Z","iopub.execute_input":"2021-10-28T12:22:26.773568Z","iopub.status.idle":"2021-10-28T12:22:26.781443Z","shell.execute_reply.started":"2021-10-28T12:22:26.773489Z","shell.execute_reply":"2021-10-28T12:22:26.780783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eval ","metadata":{}},{"cell_type":"code","source":"\ndef evaluate(model: Module, dataloader: DataLoader, prepare_batch: Callable, metrics: List[Union[str, Callable]],\n             loss_fn: Callable = None, prefix: str = 'val_', suffix: str = ''):\n    \"\"\"Evaluate a model on one or more metrics on a particular dataset\n    # Arguments\n        model: Model to evaluate\n        dataloader: Instance of torch.utils.data.DataLoader representing the dataset\n        prepare_batch: Callable to perform any desired preprocessing\n        metrics: List of metrics to evaluate the model with. Metrics must either be a named metric (see `metrics.py`) or\n            a Callable that takes predictions and ground truth labels and returns a scalar value\n        loss_fn: Loss function to calculate over the dataset\n        prefix: Prefix to prepend to the name of each metric - used to identify the dataset. Defaults to 'val_' as\n            it is typical to evaluate on a held-out validation dataset\n        suffix: Suffix to append to the name of each metric.\n    \"\"\"\n    logs = {}\n    seen = 0\n    totals = {m: 0 for m in metrics}\n    if loss_fn is not None:\n        totals['loss'] = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in dataloader:\n            x, y = prepare_batch(batch)\n            y_pred = model(x)\n\n            seen += x.shape[0]\n\n            if loss_fn is not None:\n                totals['loss'] += loss_fn(y_pred, y).item() * x.shape[0]\n\n            for m in metrics:\n                if isinstance(m, str):\n                    v = NAMED_METRICS[m](y, y_pred)\n                else:\n                    # Assume metric is a callable function\n                    v = m(y, y_pred)\n\n                totals[m] += v * x.shape[0]\n\n    for m in ['loss'] + metrics:\n        logs[prefix + m + suffix] = totals[m] / seen\n\n    return logs","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.784294Z","iopub.execute_input":"2021-10-28T12:22:26.785008Z","iopub.status.idle":"2021-10-28T12:22:26.796551Z","shell.execute_reply.started":"2021-10-28T12:22:26.784977Z","shell.execute_reply":"2021-10-28T12:22:26.795919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Call backs","metadata":{}},{"cell_type":"code","source":"class CallbackList(object):\n    \"\"\"Container abstracting a list of callbacks.\n    # Arguments\n        callbacks: List of `Callback` instances.\n    \"\"\"\n    def __init__(self, callbacks):\n        self.callbacks = [c for c in callbacks]\n\n    def set_params(self, params):\n        for callback in self.callbacks:\n            callback.set_params(params)\n\n    def set_model(self, model):\n        for callback in self.callbacks:\n            callback.set_model(model)\n\n    def on_epoch_begin(self, epoch, logs=None):\n        \"\"\"Called at the start of an epoch.\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_begin(epoch, logs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Called at the end of an epoch.\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_end(epoch, logs)\n\n    def on_batch_begin(self, batch, logs=None):\n        \"\"\"Called right before processing a batch.\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_batch_begin(batch, logs)\n\n    def on_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a batch.\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_batch_end(batch, logs)\n\n    def on_train_begin(self, logs=None):\n        \"\"\"Called at the beginning of training.\n        # Arguments\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        \"\"\"Called at the end of training.\n        # Arguments\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n\nclass Callback(object):\n    def __init__(self):\n        self.model = None\n\n    def set_params(self, params):\n        self.params = params\n\n    def set_model(self, model):\n        self.model = model\n\n    def on_epoch_begin(self, epoch, logs=None):\n        pass\n\n    def on_epoch_end(self, epoch, logs=None):\n        pass\n\n    def on_batch_begin(self, batch, logs=None):\n        pass\n\n    def on_batch_end(self, batch, logs=None):\n        pass\n\n    def on_train_begin(self, logs=None):\n        pass\n\n    def on_train_end(self, logs=None):\n        pass\n\n\nclass DefaultCallback(Callback):\n    \"\"\"Records metrics over epochs by averaging over each batch.\n    NB The metrics are calculated with a moving model\n    \"\"\"\n    def on_epoch_begin(self, batch, logs=None):\n        self.seen = 0\n        self.totals = {}\n        self.metrics = ['loss'] + self.params['metrics']\n\n    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        batch_size = logs.get('size', 1) or 1\n        self.seen += batch_size\n\n        for k, v in logs.items():\n            if k in self.totals:\n                self.totals[k] += v * batch_size\n            else:\n                self.totals[k] = v * batch_size\n\n    def on_epoch_end(self, epoch, logs=None):\n        if logs is not None:\n            for k in self.metrics:\n                if k in self.totals:\n                    # Make value available to next callbacks.\n                    logs[k] = self.totals[k] / self.seen\n\n\nclass ProgressBarLogger(Callback):\n    \"\"\"TQDM progress bar that displays the running average of loss and other metrics.\"\"\"\n    def __init__(self):\n        super(ProgressBarLogger, self).__init__()\n\n    def on_train_begin(self, logs=None):\n        self.num_batches = self.params['num_batches']\n        self.verbose = self.params['verbose']\n        self.metrics = ['loss'] + self.params['metrics']\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.target = self.num_batches\n        self.pbar = tqdm(total=self.target, desc='Epoch {}'.format(epoch))\n        self.seen = 0\n\n    def on_batch_begin(self, batch, logs=None):\n        self.log_values = {}\n\n    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        self.seen += 1\n\n        for k in self.metrics:\n            if k in logs:\n                self.log_values[k] = logs[k]\n\n        # Skip progbar update for the last batch;\n        # will be handled by on_epoch_end.\n        if self.verbose and self.seen < self.target:\n            self.pbar.update(1)\n            self.pbar.set_postfix(self.log_values)\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Update log values\n        self.log_values = {}\n        for k in self.metrics:\n            if k in logs:\n                self.log_values[k] = logs[k]\n\n        if self.verbose:\n            self.pbar.update(1)\n            self.pbar.set_postfix(self.log_values)\n\n        self.pbar.close()\n\n\nimport io as ioo\nclass CSVLogger(Callback):\n    \"\"\"Callback that streams epoch results to a csv file.\n    Supports all values that can be represented as a string,\n    including 1D iterables such as np.ndarray.\n    # Arguments\n        filename: filename of the csv file, e.g. 'run/log.csv'.\n        separator: string used to separate elements in the csv file.\n        append: True: append if file exists (useful for continuing\n            training). False: overwrite existing file,\n    \"\"\"\n\n    def __init__(self, filename, separator=',', append=False):\n        self.sep = separator\n        self.filename = filename\n        self.append = append\n        self.writer = None\n        self.keys = None\n        self.append_header = True\n        self.file_flags = ''\n        self._open_args = {'newline': '\\n'}\n        super(CSVLogger, self).__init__()\n\n    def on_train_begin(self, logs=None):\n        if self.append:\n            if os.path.exists(self.filename):\n                with open(self.filename, 'r' + self.file_flags) as f:\n                    self.append_header = not bool(len(f.readline()))\n            mode = 'a'\n        else:\n            mode = 'w'\n\n        self.csv_file = ioo.open(self.filename,\n                                mode + self.file_flags,\n                                **self._open_args)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        def handle_value(k):\n            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n            if isinstance(k, str):\n                return k\n            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:\n                return '\"[%s]\"' % (', '.join(map(str, k)))\n            else:\n                return k\n\n        if self.keys is None:\n            self.keys = sorted(logs.keys())\n\n        if not self.writer:\n            class CustomDialect(csv.excel):\n                delimiter = self.sep\n            fieldnames = ['epoch'] + self.keys\n            self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=fieldnames,\n                                         dialect=CustomDialect)\n            if self.append_header:\n                self.writer.writeheader()\n\n        row_dict = OrderedDict({'epoch': epoch})\n        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()\n\n    def on_train_end(self, logs=None):\n        self.csv_file.close()\n        self.writer = None\n\n\nclass EvaluateMetrics(Callback):\n    \"\"\"Evaluates metrics on a dataset after every epoch.\n    # Argments\n        dataloader: torch.DataLoader of the dataset on which the model will be evaluated\n        prefix: Prefix to prepend to the names of the metrics when they is logged. Defaults to 'val_' but can be changed\n        if the model is to be evaluated on many datasets separately.\n        suffix: Suffix to append to the names of the metrics when they is logged.\n    \"\"\"\n    def __init__(self, dataloader, prefix='val_', suffix=''):\n        super(EvaluateMetrics, self).__init__()\n        self.dataloader = dataloader\n        self.prefix = prefix\n        self.suffix = suffix\n\n    def on_train_begin(self, logs=None):\n        self.metrics = self.params['metrics']\n        self.prepare_batch = self.params['prepare_batch']\n        self.loss_fn = self.params['loss_fn']\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs.update(\n            evaluate(self.model, self.dataloader, self.prepare_batch, self.metrics, self.loss_fn, self.prefix, self.suffix)\n        )\n\n\nclass ReduceLROnPlateau(Callback):\n    \"\"\"Reduce learning rate when a metric has stopped improving.\n    Models often benefit from reducing the learning rate by a factor\n    of 2-10 once learning stagnates. This callback monitors a\n    quantity and if no improvement is seen for a 'patience' number\n    of epochs, the learning rate is reduced.\n    # Arguments\n        monitor: quantity to be monitored.\n        factor: factor by which the learning rate will\n            be reduced. new_lr = lr * factor\n        patience: number of epochs with no improvement\n            after which learning rate will be reduced.\n        verbose: int. 0: quiet, 1: update messages.\n        mode: one of {auto, min, max}. In `min` mode,\n            lr will be reduced when the quantity\n            monitored has stopped decreasing; in `max`\n            mode it will be reduced when the quantity\n            monitored has stopped increasing; in `auto`\n            mode, the direction is automatically inferred\n            from the name of the monitored quantity.\n        min_delta: threshold for measuring the new optimum,\n            to only focus on significant changes.\n        cooldown: number of epochs to wait before resuming\n            normal operation after lr has been reduced.\n        min_lr: lower bound on the learning rate.\n    \"\"\"\n\n    def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n        super(ReduceLROnPlateau, self).__init__()\n\n        self.monitor = monitor\n        if factor >= 1.0:\n            raise ValueError('ReduceLROnPlateau does not support a factor >= 1.0.')\n        self.factor = factor\n        self.min_lr = min_lr\n        self.min_delta = min_delta\n        self.patience = patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0  # Cooldown counter.\n        self.wait = 0\n        self.best = 0\n        if mode not in ['auto', 'min', 'max']:\n            raise ValueError('Mode must be one of (auto, min, max).')\n        self.mode = mode\n        self.monitor_op = None\n\n        self._reset()\n\n    def _reset(self):\n        \"\"\"Resets wait counter and cooldown counter.\n        \"\"\"\n        if (self.mode == 'min' or\n                (self.mode == 'auto' and 'acc' not in self.monitor)):\n            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n            self.best = np.Inf\n        else:\n            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n            self.best = -np.Inf\n        self.cooldown_counter = 0\n        self.wait = 0\n\n    def on_train_begin(self, logs=None):\n        self.optimiser = self.params['optimiser']\n        self.min_lrs = [self.min_lr] * len(self.optimiser.param_groups)\n        self._reset()\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        if len(self.optimiser.param_groups) == 1:\n            logs['lr'] = self.optimiser.param_groups[0]['lr']\n        else:\n            for i, param_group in enumerate(self.optimiser.param_groups):\n                logs['lr_{}'.format(i)] = param_group['lr']\n\n        current = logs.get(self.monitor)\n\n        if self.in_cooldown():\n            self.cooldown_counter -= 1\n            self.wait = 0\n\n        if self.monitor_op(current, self.best):\n            self.best = current\n            self.wait = 0\n        elif not self.in_cooldown():\n            self.wait += 1\n            if self.wait >= self.patience:\n                self._reduce_lr(epoch)\n                self.cooldown_counter = self.cooldown\n                self.wait = 0\n\n    def _reduce_lr(self, epoch):\n        for i, param_group in enumerate(self.optimiser.param_groups):\n            old_lr = float(param_group['lr'])\n            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n            if old_lr - new_lr > self.min_delta:\n                param_group['lr'] = new_lr\n                if self.verbose:\n                    print('Epoch {:5d}: reducing learning rate'\n                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\n\n    def in_cooldown(self):\n        return self.cooldown_counter > 0\n\n\nclass ModelCheckpoint(Callback):\n    \"\"\"Save the model after every epoch.\n    `filepath` can contain named formatting options, which will be filled the value of `epoch` and keys in `logs`\n    (passed in `on_epoch_end`).\n    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model checkpoints will be saved\n    with the epoch number and the validation loss in the filename.\n    # Arguments\n        filepath: string, path to save the model file.\n        monitor: quantity to monitor.\n        verbose: verbosity mode, 0 or 1.\n        save_best_only: if `save_best_only=True`,\n            the latest best model according to\n            the quantity monitored will not be overwritten.\n        mode: one of {auto, min, max}.\n            If `save_best_only=True`, the decision\n            to overwrite the current save file is made\n            based on either the maximization or the\n            minimization of the monitored quantity. For `val_acc`,\n            this should be `max`, for `val_loss` this should\n            be `min`, etc. In `auto` mode, the direction is\n            automatically inferred from the name of the monitored quantity.\n        save_weights_only: if True, then only the model's weights will be\n            saved (`model.save_weights(filepath)`), else the full model\n            is saved (`model.save(filepath)`).\n        period: Interval (number of epochs) between checkpoints.\n    \"\"\"\n\n    def __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, mode='auto', period=1):\n        super(ModelCheckpoint, self).__init__()\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = filepath\n        self.save_best_only = save_best_only\n        self.period = period\n        self.epochs_since_last_save = 0\n\n        if mode not in ['auto', 'min', 'max']:\n            raise ValueError('Mode must be one of (auto, min, max).')\n\n        if mode == 'min':\n            self.monitor_op = np.less\n            self.best = np.Inf\n        elif mode == 'max':\n            self.monitor_op = np.greater\n            self.best = -np.Inf\n        else:\n            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n                self.monitor_op = np.greater\n                self.best = -np.Inf\n            else:\n                self.monitor_op = np.less\n\n        self.best = np.Inf\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n            if self.save_best_only:\n                current = logs.get(self.monitor)\n                if current is None:\n                    warnings.warn('Can save best model only with %s available, '\n                                  'skipping.' % (self.monitor), RuntimeWarning)\n                else:\n                    if self.monitor_op(current, self.best):\n                        if self.verbose > 0:\n                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n                                  ' saving model to %s'\n                                  % (epoch + 1, self.monitor, self.best,\n                                     current, filepath))\n                        self.best = current\n                        torch.save(self.model.state_dict(), filepath)\n                    else:\n                        if self.verbose > 0:\n                            print('\\nEpoch %05d: %s did not improve from %0.5f' %\n                                  (epoch + 1, self.monitor, self.best))\n            else:\n                if self.verbose > 0:\n                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n                torch.save(self.model.state_dict(), filepath)\n\n\nclass LearningRateScheduler(Callback):\n    \"\"\"Learning rate scheduler.\n    # Arguments\n        schedule: a function that takes an epoch index as input\n            (integer, indexed from 0) and current learning rate\n            and returns a new learning rate as output (float).\n        verbose: int. 0: quiet, 1: update messages.\n    \"\"\"\n\n    def __init__(self, schedule, verbose=0):\n        super(LearningRateScheduler, self).__init__()\n        self.schedule = schedule\n        self.verbose = verbose\n\n    def on_train_begin(self, logs=None):\n        self.optimiser = self.params['optimiser']\n\n    def on_epoch_begin(self, epoch, logs=None):\n        lrs = [self.schedule(epoch, param_group['lr']) for param_group in self.optimiser.param_groups]\n\n        if not all(isinstance(lr, (float, np.float32, np.float64)) for lr in lrs):\n            raise ValueError('The output of the \"schedule\" function '\n                             'should be float.')\n        self.set_lr(epoch, lrs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        if len(self.optimiser.param_groups) == 1:\n            logs['lr'] = self.optimiser.param_groups[0]['lr']\n        else:\n            for i, param_group in enumerate(self.optimiser.param_groups):\n                logs['lr_{}'.format(i)] = param_group['lr']\n\n    def set_lr(self, epoch, lrs):\n        for i, param_group in enumerate(self.optimiser.param_groups):\n            new_lr = lrs[i]\n            param_group['lr'] = new_lr\n            if self.verbose:\n                print('Epoch {:5d}: setting learning rate'\n                      ' of group {} to {:.4e}.'.format(epoch, i, new_lr))","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.79802Z","iopub.execute_input":"2021-10-28T12:22:26.798262Z","iopub.status.idle":"2021-10-28T12:22:26.871873Z","shell.execute_reply.started":"2021-10-28T12:22:26.798232Z","shell.execute_reply":"2021-10-28T12:22:26.871133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets ","metadata":{}},{"cell_type":"code","source":"class OmniglotDataset(Dataset):\n    def __init__(self, subset):\n        \"\"\"Dataset class representing Omniglot dataset\n        # Arguments:\n            subset: Whether the dataset represents the background or evaluation set\n        \"\"\"\n        if subset not in ('background', 'evaluation'):\n            raise(ValueError, 'subset must be one of (background, evaluation)')\n        self.subset = subset\n\n        self.df = pd.DataFrame(self.index_subset(self.subset))\n\n        # Index of dataframe has direct correspondence to item in dataset\n        self.df = self.df.assign(id=self.df.index.values)\n\n        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n        self.unique_characters = sorted(self.df['class_name'].unique())\n        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n        self.df = self.df.assign(class_id=self.df['class_name'].apply(lambda c: self.class_name_to_id[c]))\n\n        # Create dicts\n        self.datasetid_to_filepath = self.df.to_dict()['filepath']\n        self.datasetid_to_class_id = self.df.to_dict()['class_id']\n\n    def __getitem__(self, item):\n        instance = io.imread(self.datasetid_to_filepath[item])\n        # Reindex to channels first format as supported by pytorch\n        instance = instance[np.newaxis, :, :]\n\n        # Normalise to 0-1\n        instance = (instance - instance.min()) / (instance.max() - instance.min())\n\n        label = self.datasetid_to_class_id[item]\n\n        return torch.from_numpy(instance), label\n\n    def __len__(self):\n        return len(self.df)\n\n    def num_classes(self):\n        return len(self.df['class_name'].unique())\n\n    @staticmethod\n    def index_subset(subset):\n        \"\"\"Index a subset by looping through all of its files and recording relevant information.\n        # Arguments\n            subset: Name of the subset\n        # Returns\n            A list of dicts containing information about all the image files in a particular subset of the\n            Omniglot dataset dataset\n        \"\"\"\n        images = []\n        print('Indexing {}...'.format(subset))\n        # Quick first pass to find total for tqdm bar\n        subset_len = 0\n        for root, folders, files in os.walk(DATA_PATH + '/omniglot/images_{}/'.format(subset)):\n            subset_len += len([f for f in files if f.endswith('.png')])\n\n        progress_bar = tqdm(total=subset_len)\n        for root, folders, files in os.walk(DATA_PATH + '/omniglot/images_{}/'.format(subset)):\n            if len(files) == 0:\n                continue\n\n            alphabet = root.split('/')[-2]\n            class_name = '{}.{}'.format(alphabet, root.split('/')[-1])\n\n            for f in files:\n                progress_bar.update(1)\n                images.append({\n                    'subset': subset,\n                    'alphabet': alphabet,\n                    'class_name': class_name,\n                    'filepath': os.path.join(root, f)\n                })\n\n        progress_bar.close()\n        return images\n\n\n\nclass MiniImageNet(Dataset):\n    def __init__(self, subset):\n        \"\"\"Dataset class representing miniImageNet dataset\n        # Arguments:\n            subset: Whether the dataset represents the background or evaluation set\n        \"\"\"\n        if subset not in ('background', 'evaluation'):\n            raise(ValueError, 'subset must be one of (background, evaluation)')\n        self.subset = subset\n\n        self.df = pd.DataFrame(self.index_subset(self.subset))\n\n        # Index of dataframe has direct correspondence to item in dataset\n        self.df = self.df.assign(id=self.df.index.values)\n\n        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n        self.unique_characters = sorted(self.df['class_name'].unique())\n        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n        self.df = self.df.assign(class_id=self.df['class_name'].apply(lambda c: self.class_name_to_id[c]))\n\n        # Create dicts\n        self.datasetid_to_filepath = self.df.to_dict()['filepath']\n        self.datasetid_to_class_id = self.df.to_dict()['class_id']\n\n        # Setup transforms\n        self.transform = transforms.Compose([\n            transforms.CenterCrop(224),\n            transforms.Resize(84),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n\n    def __getitem__(self, item):\n        instance = Image.open(self.datasetid_to_filepath[item])\n        instance = self.transform(instance)\n        label = self.datasetid_to_class_id[item]\n        return instance, label\n\n    def __len__(self):\n        return len(self.df)\n\n    def num_classes(self):\n        return len(self.df['class_name'].unique())\n\n    @staticmethod\n    def index_subset(subset):\n        \"\"\"Index a subset by looping through all of its files and recording relevant information.\n        # Arguments\n            subset: Name of the subset\n        # Returns\n            A list of dicts containing information about all the image files in a particular subset of the\n            miniImageNet dataset\n        \"\"\"\n        images = []\n        print('Indexing {}...'.format(subset))\n        # Quick first pass to find total for tqdm bar\n        subset_len = 0\n        for root, folders, files in os.walk(DATA_PATH + '/miniImageNet/images_{}/'.format(subset)):\n            subset_len += len([f for f in files if f.endswith('.png')])\n\n        progress_bar = tqdm(total=subset_len)\n        for root, folders, files in os.walk(DATA_PATH + '/miniImageNet/images_{}/'.format(subset)):\n            if len(files) == 0:\n                continue\n\n            class_name = root.split('/')[-1]\n\n            for f in files:\n                progress_bar.update(1)\n                images.append({\n                    'subset': subset,\n                    'class_name': class_name,\n                    'filepath': os.path.join(root, f)\n                })\n\n        progress_bar.close()\n        return images\n\n\nclass DummyDataset(Dataset):\n    def __init__(self, samples_per_class=10, n_classes=10, n_features=1):\n        \"\"\"Dummy dataset for debugging/testing purposes\n        A sample from the DummyDataset has (n_features + 1) features. The first feature is the index of the sample\n        in the data and the remaining features are the class index.\n        # Arguments\n            samples_per_class: Number of samples per class in the dataset\n            n_classes: Number of distinct classes in the dataset\n            n_features: Number of extra features each sample should have.\n        \"\"\"\n        self.samples_per_class = samples_per_class\n        self.n_classes = n_classes\n        self.n_features = n_features\n\n        # Create a dataframe to be consistent with other Datasets\n        self.df = pd.DataFrame({\n            'class_id': [i % self.n_classes for i in range(len(self))]\n        })\n        self.df = self.df.assign(id=self.df.index.values)\n\n    def __len__(self):\n        return self.samples_per_class * self.n_classes\n\n    def __getitem__(self, item):\n        class_id = item % self.n_classes\n        return np.array([item] + [class_id]*self.n_features, dtype=np.float), float(class_id)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.873446Z","iopub.execute_input":"2021-10-28T12:22:26.873989Z","iopub.status.idle":"2021-10-28T12:22:26.904608Z","shell.execute_reply.started":"2021-10-28T12:22:26.873938Z","shell.execute_reply":"2021-10-28T12:22:26.903909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Core ","metadata":{}},{"cell_type":"code","source":"class NShotTaskSampler(Sampler):\n    def __init__(self,\n                 dataset: torch.utils.data.Dataset,\n                 episodes_per_epoch: int = None,\n                 n: int = None,\n                 k: int = None,\n                 q: int = None,\n                 num_tasks: int = 1,\n                 fixed_tasks: List[Iterable[int]] = None):\n        \"\"\"PyTorch Sampler subclass that generates batches of n-shot, k-way, q-query tasks.\n        Each n-shot task contains a \"support set\" of `k` sets of `n` samples and a \"query set\" of `k` sets\n        of `q` samples. The support set and the query set are all grouped into one Tensor such that the first n * k\n        samples are from the support set while the remaining q * k samples are from the query set.\n        The support and query sets are sampled such that they are disjoint i.e. do not contain overlapping samples.\n        # Arguments\n            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to generate in one epoch\n            n_shot: int. Number of samples for each class in the n-shot classification tasks.\n            k_way: int. Number of classes in the n-shot classification tasks.\n            q_queries: int. Number query samples for each class in the n-shot classification tasks.\n            num_tasks: Number of n-shot tasks to group into a single batch\n            fixed_tasks: If this argument is specified this Sampler will always generate tasks from\n                the specified classes\n        \"\"\"\n        super(NShotTaskSampler, self).__init__(dataset)\n        self.episodes_per_epoch = episodes_per_epoch\n        self.dataset = dataset\n        if num_tasks < 1:\n            raise ValueError('num_tasks must be > 1.')\n\n        self.num_tasks = num_tasks\n        # TODO: Raise errors if initialise badly\n        self.k = k\n        self.n = n\n        self.q = q\n        self.fixed_tasks = fixed_tasks\n\n        self.i_task = 0\n\n    def __len__(self):\n        return self.episodes_per_epoch\n\n    def __iter__(self):\n        for _ in range(self.episodes_per_epoch):\n            batch = []\n\n            for task in range(self.num_tasks):\n                if self.fixed_tasks is None:\n                    # Get random classes\n                    episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n                else:\n                    # Loop through classes in fixed_tasks\n                    episode_classes = self.fixed_tasks[self.i_task % len(self.fixed_tasks)]\n                    self.i_task += 1\n\n                df = self.dataset.df[self.dataset.df['class_id'].isin(episode_classes)]\n\n                support_k = {k: None for k in episode_classes}\n                for k in episode_classes:\n                    # Select support examples\n                    support = df[df['class_id'] == k].sample(self.n)\n                    support_k[k] = support\n\n                    for i, s in support.iterrows():\n                        batch.append(s['id'])\n\n                for k in episode_classes:\n                    query = df[(df['class_id'] == k) & (~df['id'].isin(support_k[k]['id']))].sample(self.q)\n                    for i, q in query.iterrows():\n                        batch.append(q['id'])\n\n            yield np.stack(batch)\n\n\nclass EvaluateFewShot(Callback):\n    \"\"\"Evaluate a network on  an n-shot, k-way classification tasks after every epoch.\n    # Arguments\n        eval_fn: Callable to perform few-shot classification. Examples include `proto_net_episode`,\n            `matching_net_episode` and `meta_gradient_step` (MAML).\n        num_tasks: int. Number of n-shot classification tasks to evaluate the model with.\n        n_shot: int. Number of samples for each class in the n-shot classification tasks.\n        k_way: int. Number of classes in the n-shot classification tasks.\n        q_queries: int. Number query samples for each class in the n-shot classification tasks.\n        task_loader: Instance of NShotWrapper class\n        prepare_batch: function. The preprocessing function to apply to samples from the dataset.\n        prefix: str. Prefix to identify dataset.\n    \"\"\"\n\n    def __init__(self,\n                 eval_fn: Callable,\n                 num_tasks: int,\n                 n_shot: int,\n                 k_way: int,\n                 q_queries: int,\n                 taskloader: torch.utils.data.DataLoader,\n                 prepare_batch: Callable,\n                 prefix: str = 'val_',\n                 **kwargs):\n        super(EvaluateFewShot, self).__init__()\n        self.eval_fn = eval_fn\n        self.num_tasks = num_tasks\n        self.n_shot = n_shot\n        self.k_way = k_way\n        self.q_queries = q_queries\n        self.taskloader = taskloader\n        self.prepare_batch = prepare_batch\n        self.prefix = prefix\n        self.kwargs = kwargs\n        self.metric_name = f'{self.prefix}{self.n_shot}-shot_{self.k_way}-way_acc'\n\n    def on_train_begin(self, logs=None):\n        self.loss_fn = self.params['loss_fn']\n        self.optimiser = self.params['optimiser']\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        seen = 0\n        totals = {'loss': 0, self.metric_name: 0}\n        for batch_index, batch in enumerate(self.taskloader):\n            x, y = self.prepare_batch(batch)\n\n            loss, y_pred = self.eval_fn(\n                self.model,\n                self.optimiser,\n                self.loss_fn,\n                x,\n                y,\n                n_shot=self.n_shot,\n                k_way=self.k_way,\n                q_queries=self.q_queries,\n                train=False,\n                **self.kwargs\n            )\n\n            seen += y_pred.shape[0]\n\n            totals['loss'] += loss.item() * y_pred.shape[0]\n            totals[self.metric_name] += categorical_accuracy(y, y_pred) * y_pred.shape[0]\n\n        logs[self.prefix + 'loss'] = totals['loss'] / seen\n        logs[self.metric_name] = totals[self.metric_name] / seen\n\n\ndef prepare_nshot_task(n: int, k: int, q: int) -> Callable:\n    \"\"\"Typical n-shot task preprocessing.\n    # Arguments\n        n: Number of samples for each class in the n-shot classification task\n        k: Number of classes in the n-shot classification task\n        q: Number of query samples for each class in the n-shot classification task\n    # Returns\n        prepare_nshot_task_: A Callable that processes a few shot tasks with specified n, k and q\n    \"\"\"\n    def prepare_nshot_task_(batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Create 0-k label and move to GPU.\n        TODO: Move to arbitrary device\n        \"\"\"\n        x, y = batch\n        x = x.double().cuda()\n        # Create dummy 0-(num_classes - 1) label\n        y = create_nshot_task_label(k, q).cuda()\n        return x, y\n\n    return prepare_nshot_task_\n\n\ndef create_nshot_task_label(k: int, q: int) -> torch.Tensor:\n    \"\"\"Creates an n-shot task label.\n    Label has the structure:\n        [0]*q + [1]*q + ... + [k-1]*q\n    # TODO: Test this\n    # Arguments\n        k: Number of classes in the n-shot classification task\n        q: Number of query samples for each class in the n-shot classification task\n    # Returns\n        y: Label vector for n-shot task of shape [q * k, ]\n    \"\"\"\n    y = torch.arange(0, k, 1 / q).long()\n    return y\n\n## Matching ","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.907845Z","iopub.execute_input":"2021-10-28T12:22:26.908077Z","iopub.status.idle":"2021-10-28T12:22:26.93527Z","shell.execute_reply.started":"2021-10-28T12:22:26.908044Z","shell.execute_reply":"2021-10-28T12:22:26.934471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef matching_net_episode(model: Module,\n                         optimiser: Optimizer,\n                         loss_fn: Loss,\n                         x: torch.Tensor,\n                         y: torch.Tensor,\n                         n_shot: int,\n                         k_way: int,\n                         q_queries: int,\n                         distance: str,\n                         fce: bool,\n                         train: bool):\n    \"\"\"Performs a single training episode for a Matching Network.\n    # Arguments\n        model: Matching Network to be trained.\n        optimiser: Optimiser to calculate gradient step from loss\n        loss_fn: Loss function to calculate between predictions and outputs\n        x: Input samples of few shot classification task\n        y: Input labels of few shot classification task\n        n_shot: Number of examples per class in the support set\n        k_way: Number of classes in the few shot classification task\n        q_queries: Number of examples per class in the query set\n        distance: Distance metric to use when calculating distance between support and query set samples\n        fce: Whether or not to us fully conditional embeddings\n        train: Whether (True) or not (False) to perform a parameter update\n    # Returns\n        loss: Loss of the Matching Network on this task\n        y_pred: Predicted class probabilities for the query set on this task\n    \"\"\"\n    if train:\n        # Zero gradients\n        model.train()\n        optimiser.zero_grad()\n    else:\n        model.eval()\n\n    # Embed all samples\n    embeddings = model.encoder(x)\n#     print('embedding : ',embeddings.shape)\n\n    # Samples are ordered by the NShotWrapper class as follows:\n    # k lots of n support samples from a particular class\n    # k lots of q query samples from those classes\n    support = embeddings[:n_shot * k_way]\n    queries = embeddings[n_shot * k_way:(n_shot * k_way)+(q_queries * k_way)]\n\n    # Optionally apply full context embeddings\n    if fce:\n        # LSTM requires input of shape (seq_len, batch, input_size). `support` is of\n        # shape (k_way * n_shot, embedding_dim) and we want the LSTM to treat the\n        # support set as a sequence so add a single dimension to transform support set\n        # to the shape (k_way * n_shot, 1, embedding_dim) and then remove the batch dimension\n        # afterwards\n\n        # Calculate the fully conditional embedding, g, for support set samples as described\n        # in appendix A.2 of the paper. g takes the form of a bidirectional LSTM with a\n        # skip connection from inputs to outputs\n#         print(support.shape)\n        support, _, _ = model.g(support.unsqueeze(1))\n        support = support.squeeze(1)\n\n        # Calculate the fully conditional embedding, f, for the query set samples as described\n        # in appendix A.1 of the paper.\n        queries = model.f(support, queries)\n\n    # Efficiently calculate distance between all queries and all prototypes\n    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n    distances = pairwise_distances(queries, support, distance)\n\n    # Calculate \"attention\" as softmax over support-query distances\n    attention = (-distances).softmax(dim=1)\n    \n#     print('queries.shape : ',queries.shape, ' support.shape : ',support.shape,' attention.shape : '  ,attention.shape,' Embedding.shape : ',embeddings.shape,' x.shape : ',x.shape)\n    \n    # Calculate predictions as in equation (1) from Matching Networks\n    # y_hat = \\sum_{i=1}^{k} a(x_hat, x_i) y_i\n    y_pred = matching_net_predictions(attention, n_shot, k_way, q_queries)\n\n    # Calculated loss with negative log likelihood\n    # Clip predictions for numerical stability\n    clipped_y_pred = y_pred.clamp(EPSILON, 1 - EPSILON)\n    loss = loss_fn(clipped_y_pred.log(), y)\n\n    if train:\n        # Backpropagate gradients\n        loss.backward()\n        # I found training to be quite unstable so I clip the norm\n        # of the gradient to be at most 1\n        clip_grad_norm_(model.parameters(), 1)\n        # Take gradient step\n        optimiser.step()\n\n    return loss, y_pred\n\n\ndef matching_net_predictions(attention: torch.Tensor, n: int, k: int, q: int) -> torch.Tensor:\n    \"\"\"Calculates Matching Network predictions based on equation (1) of the paper.\n    The predictions are the weighted sum of the labels of the support set where the\n    weights are the \"attentions\" (i.e. softmax over query-support distances) pointing\n    from the query set samples to the support set samples.\n    # Arguments\n        attention: torch.Tensor containing softmax over query-support distances.\n            Should be of shape (q * k, k * n)\n        n: Number of support set samples per class, n-shot\n        k: Number of classes in the episode, k-way\n        q: Number of query samples per-class\n    # Returns\n        y_pred: Predicted class probabilities\n    \"\"\"\n#     print('q : ',q,' k : ',k,' n : ',n,' attention.shape : ',attention.shape)\n    if attention.shape != (q * k, k * n):\n        raise(ValueError(f'Expecting attention Tensor to have shape (q * k, k * n) = ({q * k, k * n})'))\n\n    # Create one hot label vector for the support set\n    y_onehot = torch.zeros(k * n, k)\n\n    # Unsqueeze to force y to be of shape (K*n, 1) as this\n    # is needed for .scatter()\n    y = create_nshot_task_label(k, n).unsqueeze(-1)\n    y_onehot = y_onehot.scatter(1, y, 1)\n\n    y_pred = torch.mm(attention, y_onehot.cuda().double())\n\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.936639Z","iopub.execute_input":"2021-10-28T12:22:26.936879Z","iopub.status.idle":"2021-10-28T12:22:26.952811Z","shell.execute_reply.started":"2021-10-28T12:22:26.936846Z","shell.execute_reply":"2021-10-28T12:22:26.951957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models ","metadata":{}},{"cell_type":"code","source":"\n##########\n# Layers #\n##########\nclass Flatten(nn.Module):\n    \"\"\"Converts N-dimensional Tensor of shape [batch_size, d1, d2, ..., dn] to 2-dimensional Tensor\n    of shape [batch_size, d1*d2*...*dn].\n    # Arguments\n        input: Input tensor\n    \"\"\"\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\nclass GlobalMaxPool1d(nn.Module):\n    \"\"\"Performs global max pooling over the entire length of a batched 1D tensor\n    # Arguments\n        input: Input tensor\n    \"\"\"\n    def forward(self, input):\n        return nn.functional.max_pool1d(input, kernel_size=input.size()[2:]).view(-1, input.size(1))\n\n\nclass GlobalAvgPool2d(nn.Module):\n    \"\"\"Performs global average pooling over the entire height and width of a batched 2D tensor\n    # Arguments\n        input: Input tensor\n    \"\"\"\n    def forward(self, input):\n        return nn.functional.avg_pool2d(input, kernel_size=input.size()[2:]).view(-1, input.size(1))\n\n\ndef conv_block(in_channels: int, out_channels: int) -> nn.Module:\n    \"\"\"Returns a Module that performs 3x3 convolution, ReLu activation, 2x2 max pooling.\n    # Arguments\n        in_channels:\n        out_channels:\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2)\n    )\n\n\ndef functional_conv_block(x: torch.Tensor, weights: torch.Tensor, biases: torch.Tensor,\n                          bn_weights, bn_biases) -> torch.Tensor:\n    \"\"\"Performs 3x3 convolution, ReLu activation, 2x2 max pooling in a functional fashion.\n    # Arguments:\n        x: Input Tensor for the conv block\n        weights: Weights for the convolutional block\n        biases: Biases for the convolutional block\n        bn_weights:\n        bn_biases:\n    \"\"\"\n    x = F.conv2d(x, weights, biases, padding=1)\n    x = F.batch_norm(x, running_mean=None, running_var=None, weight=bn_weights, bias=bn_biases, training=True)\n    x = F.relu(x)\n    x = F.max_pool2d(x, kernel_size=2, stride=2)\n    return x\n\n\n##########\n# Models #\n##########\ndef get_few_shot_encoder(num_input_channels=1) -> nn.Module:\n    \"\"\"Creates a few shot encoder as used in Matching and Prototypical Networks\n    # Arguments:\n        num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n            miniImageNet = 3\n    \"\"\"\n    return nn.Sequential(\n        conv_block(num_input_channels, 64),\n        conv_block(64, 64),\n        conv_block(64, 64),\n        conv_block(64, 64),\n        conv_block(64, 64),\n        conv_block(64, 64),\n        Flatten(),\n    )\n\n\nclass FewShotClassifier(nn.Module):\n    def __init__(self, num_input_channels: int, k_way: int, final_layer_size: int = 64):\n        \"\"\"Creates a few shot classifier as used in MAML.\n        This network should be identical to the one created by `get_few_shot_encoder` but with a\n        classification layer on top.\n        # Arguments:\n            num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n                miniImageNet = 3\n            k_way: Number of classes the model will discriminate between\n            final_layer_size: 64 for Omniglot, 1600 for miniImageNet\n        \"\"\"\n        super(FewShotClassifier, self).__init__()\n        self.conv1 = conv_block(num_input_channels, 64)\n        self.conv2 = conv_block(64, 64)\n        self.conv3 = conv_block(64, 64)\n        self.conv4 = conv_block(64, 64)\n\n        self.logits = nn.Linear(final_layer_size, k_way)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        x = x.view(x.size(0), -1)\n\n        return self.logits(x)\n\n    def functional_forward(self, x, weights):\n        \"\"\"Applies the same forward pass using PyTorch functional operators using a specified set of weights.\"\"\"\n\n        for block in [1, 2, 3, 4]:\n            x = functional_conv_block(x, weights[f'conv{block}.0.weight'], weights[f'conv{block}.0.bias'],\n                                      weights.get(f'conv{block}.1.weight'), weights.get(f'conv{block}.1.bias'))\n\n        x = x.view(x.size(0), -1)\n\n        x = F.linear(x, weights['logits.weight'], weights['logits.bias'])\n\n        return x\n\n\nclass MatchingNetwork(nn.Module):\n    def __init__(self, n: int, k: int, q: int, fce: bool, num_input_channels: int,\n                 lstm_layers: int, lstm_input_size: int, unrolling_steps: int, device: torch.device):\n        \"\"\"Creates a Matching Network as described in Vinyals et al.\n        # Arguments:\n            n: Number of examples per class in the support set\n            k: Number of classes in the few shot classification task\n            q: Number of examples per class in the query set\n            fce: Whether or not to us fully conditional embeddings\n            num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n                miniImageNet = 3\n            lstm_layers: Number of LSTM layers in the bidrectional LSTM g that embeds the support set (fce = True)\n            lstm_input_size: Input size for the bidirectional and Attention LSTM. This is determined by the embedding\n                dimension of the few shot encoder which is in turn determined by the size of the input data. Hence we\n                have Omniglot -> 64, miniImageNet -> 1600.\n            unrolling_steps: Number of unrolling steps to run the Attention LSTM\n            device: Device on which to run computation\n        \"\"\"\n        super(MatchingNetwork, self).__init__()\n        self.n = n\n        self.k = k\n        self.q = q\n        self.fce = fce\n        self.num_input_channels = num_input_channels\n        self.encoder = get_few_shot_encoder(self.num_input_channels)\n        if self.fce:\n            self.g = BidrectionalLSTM(lstm_input_size, lstm_layers).to(device, dtype=torch.double)\n            self.f = AttentionLSTM(lstm_input_size, unrolling_steps=unrolling_steps).to(device, dtype=torch.double)\n\n    def forward(self, inputs):\n        pass\n\n\nclass BidrectionalLSTM(nn.Module):\n    def __init__(self, size: int, layers: int):\n        \"\"\"Bidirectional LSTM used to generate fully conditional embeddings (FCE) of the support set as described\n        in the Matching Networks paper.\n        # Arguments\n            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n                connection described in Appendix A.2\n            layers: Number of LSTM layers\n        \"\"\"\n        super(BidrectionalLSTM, self).__init__()\n        self.num_layers = layers\n        self.batch_size = 1\n        # Force input size and hidden size to be the same in order to implement\n        # the skip connection as described in Appendix A.1 and A.2 of Matching Networks\n        self.lstm = nn.LSTM(input_size=size,\n                            num_layers=layers,\n                            hidden_size=size,\n                            bidirectional=True)\n\n    def forward(self, inputs):\n        # Give None as initial state and Pytorch LSTM creates initial hidden states\n#         print('inputs : ',inputs.shape)\n        output, (hn, cn) = self.lstm(inputs, None)\n\n        forward_output = output[:, :, :self.lstm.hidden_size]\n        backward_output = output[:, :, self.lstm.hidden_size:]\n\n        # g(x_i, S) = h_forward_i + h_backward_i + g'(x_i) as written in Appendix A.2\n        # AKA A skip connection between inputs and outputs is used\n        output = forward_output + backward_output + inputs\n        return output, hn, cn\n\n\nclass AttentionLSTM(nn.Module):\n    def __init__(self, size: int, unrolling_steps: int):\n        \"\"\"Attentional LSTM used to generate fully conditional embeddings (FCE) of the query set as described\n        in the Matching Networks paper.\n        # Arguments\n            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n                connection described in Appendix A.2\n            unrolling_steps: Number of steps of attention over the support set to compute. Analogous to number of\n                layers in a regular LSTM\n        \"\"\"\n        super(AttentionLSTM, self).__init__()\n        self.unrolling_steps = unrolling_steps\n        self.lstm_cell = nn.LSTMCell(input_size=size,\n                                     hidden_size=size)\n\n    def forward(self, support, queries):\n        # Get embedding dimension, d\n        if support.shape[-1] != queries.shape[-1]:\n            raise(ValueError(\"Support and query set have different embedding dimension!\"))\n\n        batch_size = queries.shape[0]\n        embedding_dim = queries.shape[1]\n\n        h_hat = torch.zeros_like(queries).cuda().double()\n        c = torch.zeros(batch_size, embedding_dim).cuda().double()\n\n        for k in range(self.unrolling_steps):\n            # Calculate hidden state cf. equation (4) of appendix A.2\n            h = h_hat + queries\n\n            # Calculate softmax attentions between hidden states and support set embeddings\n            # cf. equation (6) of appendix A.2\n            attentions = torch.mm(h, support.t())\n            attentions = attentions.softmax(dim=1)\n\n            # Calculate readouts from support set embeddings cf. equation (5)\n            readout = torch.mm(attentions, support)\n\n            # Run LSTM cell cf. equation (3)\n            # h_hat, c = self.lstm_cell(queries, (torch.cat([h, readout], dim=1), c))\n            h_hat, c = self.lstm_cell(queries, (h + readout, c))\n\n        h = h_hat + queries\n\n        return h","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:27:46.47459Z","iopub.status.idle":"2021-10-28T12:27:46.475487Z","shell.execute_reply.started":"2021-10-28T12:27:46.475187Z","shell.execute_reply":"2021-10-28T12:27:46.475216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train ","metadata":{}},{"cell_type":"code","source":"def gradient_step(model: Module, optimiser: Optimizer, loss_fn: Callable, x: torch.Tensor, y: torch.Tensor, **kwargs):\n    \"\"\"Takes a single gradient step.\n    # Arguments\n        model: Model to be fitted\n        optimiser: Optimiser to calculate gradient step from loss\n        loss_fn: Loss function to calculate between predictions and outputs\n        x: Input samples\n        y: Input targets\n    \"\"\"\n    model.train()\n    optimiser.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimiser.step()\n\n    return loss, y_pred\n\n\ndef batch_metrics(model: Module, y_pred: torch.Tensor, y: torch.Tensor, metrics: List[Union[str, Callable]],\n                  batch_logs: dict):\n    \"\"\"Calculates metrics for the current training batch\n    # Arguments\n        model: Model being fit\n        y_pred: predictions for a particular batch\n        y: labels for a particular batch\n        batch_logs: Dictionary of logs for the current batch\n    \"\"\"\n    model.eval()\n    for m in metrics:\n        if isinstance(m, str):\n            batch_logs[m] = NAMED_METRICS[m](y, y_pred)\n        else:\n            # Assume metric is a callable function\n            batch_logs = m(y, y_pred)\n\n    return batch_logs\n\n\ndef fit(model: Module, optimiser: Optimizer, loss_fn: Callable, epochs: int, dataloader: DataLoader,\n        prepare_batch: Callable, metrics: List[Union[str, Callable]] = None, callbacks: List[Callback] = None,\n        verbose: bool =True, fit_function: Callable = gradient_step, fit_function_kwargs: dict = {}):\n    \"\"\"Function to abstract away training loop.\n    The benefit of this function is that allows training scripts to be much more readable and allows for easy re-use of\n    common training functionality provided they are written as a subclass of voicemap.Callback (following the\n    Keras API).\n    # Arguments\n        model: Model to be fitted.\n        optimiser: Optimiser to calculate gradient step from loss\n        loss_fn: Loss function to calculate between predictions and outputs\n        epochs: Number of epochs of fitting to be performed\n        dataloader: `torch.DataLoader` instance to fit the model to\n        prepare_batch: Callable to perform any desired preprocessing\n        metrics: Optional list of metrics to evaluate the model with\n        callbacks: Additional functionality to incorporate into training such as logging metrics to csv, model\n            checkpointing, learning rate scheduling etc... See voicemap.callbacks for more.\n        verbose: All print output is muted if this argument is `False`\n        fit_function: Function for calculating gradients. Leave as default for simple supervised training on labelled\n            batches. For more complex training procedures (meta-learning etc...) you will need to write your own\n            fit_function\n        fit_function_kwargs: Keyword arguments to pass to `fit_function`\n    \"\"\"\n    # Determine number of samples:\n    num_batches = len(dataloader)\n    batch_size = dataloader.batch_size\n\n    callbacks = CallbackList([DefaultCallback(), ] + (callbacks or []) + [ProgressBarLogger(), ])\n    callbacks.set_model(model)\n    callbacks.set_params({\n        'num_batches': num_batches,\n        'batch_size': batch_size,\n        'verbose': verbose,\n        'metrics': (metrics or []),\n        'prepare_batch': prepare_batch,\n        'loss_fn': loss_fn,\n        'optimiser': optimiser\n    })\n\n    if verbose:\n        print('Begin training...')\n\n    callbacks.on_train_begin()\n\n    for epoch in range(1, epochs+1):\n        callbacks.on_epoch_begin(epoch)\n\n        epoch_logs = {}\n        for batch_index, batch in enumerate(dataloader):\n            batch_logs = dict(batch=batch_index, size=(batch_size or 1))\n\n            callbacks.on_batch_begin(batch_index, batch_logs)\n\n            x, y = prepare_batch(batch)\n#             print('X: ',x.shape,' Y: ',y.shape)\n            loss, y_pred = fit_function(model, optimiser, loss_fn, x, y, **fit_function_kwargs)\n            batch_logs['loss'] = loss.item()\n\n            # Loops through all metrics\n            batch_logs = batch_metrics(model, y_pred, y, metrics, batch_logs)\n\n            callbacks.on_batch_end(batch_index, batch_logs)\n\n        # Run on epoch end\n        \n        callbacks.on_epoch_end(epoch, epoch_logs)\n\n    # Run on train end\n    if verbose:\n        print('Finished.')\n\n    callbacks.on_train_end()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:26.990719Z","iopub.execute_input":"2021-10-28T12:22:26.991228Z","iopub.status.idle":"2021-10-28T12:22:27.009142Z","shell.execute_reply.started":"2021-10-28T12:22:26.991191Z","shell.execute_reply":"2021-10-28T12:22:27.008457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data","metadata":{}},{"cell_type":"code","source":"from distutils.dir_util import copy_tree\nfrom_dir1='../input/data-path/Omniglot_Raw/images_background/images_background'\nfrom_dir2='../input/data-path/Omniglot_Raw/images_evaluation/images_evaluation'\nto_dir='./omniglot'\nos.makedirs('./omniglot',exist_ok=True)\nos.makedirs('./omniglot/images_background',exist_ok=True)\nos.makedirs('./omniglot/images_evaluation',exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:27.010433Z","iopub.execute_input":"2021-10-28T12:22:27.010703Z","iopub.status.idle":"2021-10-28T12:22:27.023278Z","shell.execute_reply.started":"2021-10-28T12:22:27.010668Z","shell.execute_reply":"2021-10-28T12:22:27.022368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_tree(from_dir1,'./omniglot/images_background')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:22:27.024792Z","iopub.execute_input":"2021-10-28T12:22:27.025279Z","iopub.status.idle":"2021-10-28T12:24:36.278902Z","shell.execute_reply.started":"2021-10-28T12:22:27.025247Z","shell.execute_reply":"2021-10-28T12:24:36.278145Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_tree(from_dir2,'./omniglot/images_evaluation')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:24:36.280223Z","iopub.execute_input":"2021-10-28T12:24:36.280601Z","iopub.status.idle":"2021-10-28T12:26:03.934235Z","shell.execute_reply.started":"2021-10-28T12:24:36.280566Z","shell.execute_reply":"2021-10-28T12:26:03.932342Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage import io\nfrom skimage import transform\nimport skimage\nimport zipfile\nimport shutil\nimport os\n\n\n\n# Parameters\n# dataset_zip_files = ['images_background.zip', 'images_evaluation.zip']\nraw_omniglot_location = '../input/data-path'+ '/Omniglot_Raw/'\nprepared_omniglot_location = './omniglot'\noutput_shape = (28, 28)\n\n\ndef handle_characters(alphabet_folder, character_folder, rotate):\n    for root, _, character_images in os.walk(character_folder):\n        character_name = root.split('/')[-1]\n        mkdir(f'{alphabet_folder}.{rotate}/{character_name}')\n        for img_path in character_images:\n            # print(root+'/'+img_path)\n            img = io.imread(root+'/'+img_path)\n            img = transform.rotate(img, angle=rotate)\n            img = transform.resize(img, output_shape, anti_aliasing=True)\n            img = (img - img.min()) / (img.max() - img.min())\n            # print(img.min(), img.max())\n            # print(f'{alphabet_folder}.{rotate}/{character_name}/{img_path}')\n            io.imsave(f'{alphabet_folder}.{rotate}/{character_name}/{img_path}', img)\n            # return\n\n\ndef handle_alphabet(folder):\n    print('{}...'.format(folder.split('/')[-1]))\n    for rotate in [0, 90, 180, 270]:\n        # Create new folders for each augmented alphabet\n        mkdir(f'{folder}.{rotate}')\n        for root, character_folders, _ in os.walk(folder):\n            for character_folder in character_folders:\n                # For each character folder in an alphabet rotate and resize all of the images and save\n                # to the new folder\n                handle_characters(folder, root + '/' + character_folder, rotate)\n                # return\n\n    # Delete original alphabet\n    rmdir(folder)\n\n\n# Clean up previous extraction\n# rmdir(prepared_omniglot_location)\n# mkdir(prepared_omniglot_location)\n\n\n# Unzip dataset\n# for root, _, files in os.walk(raw_omniglot_location):\n#     for f in files:\n#         if f in dataset_zip_files:\n#             print('Unzipping {}...'.format(f))\n#             zip_ref = zipfile.ZipFile(root + f, 'r')\n#             zip_ref.extractall(prepared_omniglot_location)\n#             zip_ref.close()\n\n\nprint('Processing background set...')\nfor root, alphabets, _ in os.walk('./omniglot'):\n    for alphabet in sorted(alphabets):\n        handle_alphabet(root + alphabet)\n\nprint('Processing evaluation set...')\nfor root, alphabets, _ in os.walk('./omniglot'):\n    for alphabet in sorted(alphabets):\n        handle_alphabet(root + alphabet)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:26:03.935662Z","iopub.execute_input":"2021-10-28T12:26:03.936098Z","iopub.status.idle":"2021-10-28T12:26:08.528179Z","shell.execute_reply.started":"2021-10-28T12:26:03.93606Z","shell.execute_reply":"2021-10-28T12:26:08.527493Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching nets ","metadata":{}},{"cell_type":"code","source":"import sys\nsys.argv=['']\ndel sys","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:26:08.529293Z","iopub.execute_input":"2021-10-28T12:26:08.529558Z","iopub.status.idle":"2021-10-28T12:26:08.533344Z","shell.execute_reply.started":"2021-10-28T12:26:08.529522Z","shell.execute_reply":"2021-10-28T12:26:08.532658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"setup_dirs()\nassert torch.cuda.is_available()\ndevice = torch.device('cuda')\ntorch.backends.cudnn.benchmark = True\n\n\n##############\n# Parameters #\n##############\nparser = argparse.ArgumentParser()\nparser.add_argument('--dataset',default='omniglot')\nparser.add_argument('--fce',default='True', type=lambda x: x.lower()[0] == 't')  # Quick hack to extract boolean\nparser.add_argument('--distance', default='cosine')\nparser.add_argument('--n-train', default=5, type=int)\nparser.add_argument('--n-test', default=5, type=int)\nparser.add_argument('--k-train', default=5, type=int)\nparser.add_argument('--k-test', default=5, type=int)\nparser.add_argument('--q-train', default=5, type=int)\nparser.add_argument('--q-test', default=5, type=int)\nparser.add_argument('--lstm-layers', default=3, type=int)\nparser.add_argument('--unrolling-steps', default=2, type=int)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:26:08.534565Z","iopub.execute_input":"2021-10-28T12:26:08.534797Z","iopub.status.idle":"2021-10-28T12:26:08.589098Z","shell.execute_reply.started":"2021-10-28T12:26:08.534766Z","shell.execute_reply":"2021-10-28T12:26:08.588442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:26:08.590411Z","iopub.execute_input":"2021-10-28T12:26:08.59075Z","iopub.status.idle":"2021-10-28T12:26:08.595707Z","shell.execute_reply.started":"2021-10-28T12:26:08.590705Z","shell.execute_reply":"2021-10-28T12:26:08.595014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = parser.parse_args()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:26:08.596991Z","iopub.execute_input":"2021-10-28T12:26:08.59744Z","iopub.status.idle":"2021-10-28T12:26:08.605114Z","shell.execute_reply.started":"2021-10-28T12:26:08.597385Z","shell.execute_reply":"2021-10-28T12:26:08.604094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nevaluation_episodes = 100\nepisodes_per_epoch = 100\n\nif args.dataset == 'omniglot':\n    n_epochs = 20\n    dataset_class = OmniglotDataset\n    num_input_channels = 1\n    lstm_input_size = 64\nelif args.dataset == 'miniImageNet':\n    n_epochs = 200\n    dataset_class = MiniImageNet\n    num_input_channels = 3\n    lstm_input_size = 1600\nelse:\n    raise(ValueError, 'Unsupported dataset')\n\nparam_str = f'{args.dataset}_n={args.n_train}_k={args.k_train}_q={args.q_train}_' \\\n            f'nv={args.n_test}_kv={args.k_test}_qv={args.q_test}_'\\\n            f'dist={args.distance}_fce={args.fce}'\n\n\n#########\n# Model #\n#########\nmodel = MatchingNetwork(args.n_train, args.k_train, args.q_train, args.fce, num_input_channels,\n                        lstm_layers=args.lstm_layers,\n                        lstm_input_size=lstm_input_size,\n                        unrolling_steps=args.unrolling_steps,\n                        device=device)\nmodel.to(device, dtype=torch.double)\n\n\n###################\n# Create datasets #\n###################\nbackground = dataset_class('background')\nbackground_taskloader = DataLoader(\n    background,\n    batch_sampler=NShotTaskSampler(background, episodes_per_epoch, args.n_train, args.k_train, args.q_train),\n    num_workers=4\n)\nevaluation = dataset_class('evaluation')\nevaluation_taskloader = DataLoader(\n    evaluation,\n    batch_sampler=NShotTaskSampler(evaluation, episodes_per_epoch, args.n_test, args.k_test, args.q_test),\n    num_workers=4\n)\n\n\n############\n# Training #\n############\nprint(f'Training Matching Network on {args.dataset}...')\noptimiser = Adam(model.parameters(), lr=1e-3)\nloss_fn = torch.nn.NLLLoss().cuda()\n\n\ncallbacks = [\n    EvaluateFewShot(\n        eval_fn=matching_net_episode,\n        num_tasks=evaluation_episodes,\n        n_shot=args.n_test,\n        k_way=args.k_test,\n        q_queries=args.q_test,\n        taskloader=evaluation_taskloader,\n        prepare_batch=prepare_nshot_task(args.n_test, args.k_test, args.q_test),\n        fce=args.fce,\n        distance=args.distance\n    ),\n    ModelCheckpoint(\n        filepath=PATH + f'/models/matching_nets/{param_str}.pth',\n        monitor=f'val_{args.n_test}-shot_{args.k_test}-way_acc',\n        # monitor=f'val_loss',\n    ),\n    ReduceLROnPlateau(patience=20, factor=0.5, monitor=f'val_{args.n_test}-shot_{args.k_test}-way_acc'),\n    CSVLogger(PATH + f'/logs/matching_nets/{param_str}.csv'),\n]\n\nfit(\n    model,\n    optimiser,\n    loss_fn,\n    epochs=n_epochs,\n    dataloader=background_taskloader,\n    prepare_batch=prepare_nshot_task(args.n_train, args.k_train, args.q_train),\n    callbacks=callbacks,\n    metrics=['categorical_accuracy'],\n    fit_function=matching_net_episode,\n    fit_function_kwargs={'n_shot': args.n_train, 'k_way': args.k_train, 'q_queries': args.q_train, 'train': True,\n                         'fce': args.fce, 'distance': args.distance}\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:34:16.603503Z","iopub.execute_input":"2021-10-28T12:34:16.603934Z","iopub.status.idle":"2021-10-28T12:38:18.381732Z","shell.execute_reply.started":"2021-10-28T12:34:16.603896Z","shell.execute_reply":"2021-10-28T12:38:18.37947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sars cov","metadata":{}},{"cell_type":"code","source":"## Split into train,val,test\n\n!pip install split-folders","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:38:18.384052Z","iopub.execute_input":"2021-10-28T12:38:18.384897Z","iopub.status.idle":"2021-10-28T12:38:25.633853Z","shell.execute_reply.started":"2021-10-28T12:38:18.384857Z","shell.execute_reply":"2021-10-28T12:38:25.633006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import splitfolders\nsplitfolders.ratio('../input/sarscov2-ctscan-dataset',output=\"output\",seed=1337 ,ratio=(.5,.5))","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:38:25.63731Z","iopub.execute_input":"2021-10-28T12:38:25.637556Z","iopub.status.idle":"2021-10-28T12:38:40.102446Z","shell.execute_reply.started":"2021-10-28T12:38:25.637528Z","shell.execute_reply":"2021-10-28T12:38:40.101697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('./output')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:43:12.206175Z","iopub.execute_input":"2021-10-28T12:43:12.206457Z","iopub.status.idle":"2021-10-28T12:43:12.213849Z","shell.execute_reply.started":"2021-10-28T12:43:12.206422Z","shell.execute_reply":"2021-10-28T12:43:12.213105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torchvision.transforms as tt\nclass sarscov(Dataset):\n    def __init__(self,subset):\n        \"\"\"Dataset class representing Sars Cov2 dataset\n        # Arguments:\n            subset: Whether the dataset represents the background or evaluation set\n        \"\"\"\n        if subset not in ('train', 'test'):\n            raise(ValueError, 'subset must be one of (train, test)')\n        self.subset = subset\n\n        self.df = pd.DataFrame(self.index_subset(self.subset))\n\n        # Index of dataframe has direct correspondence to item in dataset\n        self.df = self.df.assign(id=self.df.index.values)\n\n        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n        self.unique_characters = sorted(self.df['class_name'].unique())\n        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n        self.df = self.df.assign(class_id=self.df['class_name'].apply(lambda c: self.class_name_to_id[c]))\n\n        # Create dicts\n        self.datasetid_to_filepath = self.df.to_dict()['filepath']\n        self.datasetid_to_class_id = self.df.to_dict()['class_id']\n        self.transform = transforms.Compose([\n            transforms.Resize((105,105)),\n            transforms.ToTensor(),\n#             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n#                                  std=[0.229, 0.224, 0.225])\n        ])\n\n    def __getitem__(self, item):\n        instance = Image.open(self.datasetid_to_filepath[item])\n        instance = self.transform(instance)\n        instance=instance[0,:,:]\n        instance=torch.unsqueeze(instance,0)\n#         print(instance.shape)\n        label = self.datasetid_to_class_id[item]\n        \n        return instance, label\n\n    def __len__(self):\n        return len(self.df)\n\n    def num_classes(self):\n        return len(self.df['class_name'].unique())\n\n    @staticmethod\n    def index_subset(subset):\n        \"\"\"Index a subset by looping through all of its files and recording relevant information.\n        # Arguments\n            None\n        # Returns\n            A list of dicts containing information about all the image files in a particular subset of the\n            sars cov2 dataset\n        \"\"\"\n        images = []\n        print('Indexing {}...'.format('total'))\n        # Quick first pass to find total for tqdm bar\n        subset_len = 0\n        for root, folders, files in os.walk('./output/train'):\n            subset_len += len([f for f in files if f.endswith('.png')])\n\n        progress_bar = tqdm(total=subset_len)\n        for root, folders, files in os.walk('./output/val'):\n            if len(files) == 0:\n                continue\n\n            alphabet = root.split('/')[-2]\n            class_name = '{}.{}'.format(alphabet, root.split('/')[-1])\n#             print(class_name,alphabet)\n\n            for f in files:\n                progress_bar.update(1)\n                images.append({\n                    'subset': subset,\n                    'alphabet': alphabet,\n                    'class_name': class_name,\n                    'filepath': os.path.join(root, f)\n                })\n\n        progress_bar.close()\n        return images","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:46:56.506634Z","iopub.execute_input":"2021-10-28T12:46:56.507048Z","iopub.status.idle":"2021-10-28T12:46:56.52413Z","shell.execute_reply.started":"2021-10-28T12:46:56.507012Z","shell.execute_reply":"2021-10-28T12:46:56.523457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.argv=['']\ndel sys","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:47:00.312789Z","iopub.execute_input":"2021-10-28T12:47:00.31305Z","iopub.status.idle":"2021-10-28T12:47:00.316855Z","shell.execute_reply.started":"2021-10-28T12:47:00.313022Z","shell.execute_reply":"2021-10-28T12:47:00.315845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"setup_dirs()\nassert torch.cuda.is_available()\ndevice = torch.device('cuda')\ntorch.backends.cudnn.benchmark = True\n\n\n##############\n# Parameters #\n##############\nparser = argparse.ArgumentParser()\nparser.add_argument('--dataset',default='omniglot')\nparser.add_argument('--fce',default='True', type=lambda x: x.lower()[0] == 't')  # Quick hack to extract boolean\nparser.add_argument('--distance', default='cosine')\nparser.add_argument('--n-train', default=5, type=int)\nparser.add_argument('--n-test', default=5, type=int)\nparser.add_argument('--k-train', default=2, type=int)\nparser.add_argument('--k-test', default=2, type=int)\nparser.add_argument('--q-train', default=5, type=int)\nparser.add_argument('--q-test', default=5, type=int)\nparser.add_argument('--lstm-layers', default=3, type=int)\nparser.add_argument('--unrolling-steps', default=2, type=int)\n\nargs = parser.parse_args()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:47:01.440755Z","iopub.execute_input":"2021-10-28T12:47:01.44129Z","iopub.status.idle":"2021-10-28T12:47:01.451282Z","shell.execute_reply.started":"2021-10-28T12:47:01.441252Z","shell.execute_reply":"2021-10-28T12:47:01.45043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###################\n# Create datasets #\n###################\nbackground = sarscov('train')\nbackground_taskloader = DataLoader(\n    background,\n    batch_sampler=NShotTaskSampler(background, episodes_per_epoch, args.n_train, args.k_train, args.q_train),\n    num_workers=4\n)\nevaluation = sarscov('test')\nevaluation_taskloader = DataLoader(\n    evaluation,\n    batch_sampler=NShotTaskSampler(evaluation, episodes_per_epoch, args.n_test, args.k_test, args.q_test),\n    num_workers=4\n)\n\n\n############\n# Training #\n############\nprint(f'Training Matching Network on {args.dataset}...')\noptimiser = Adam(model.parameters(), lr=1e-3)\nloss_fn = torch.nn.NLLLoss().cuda()\n\n\ncallbacks = [\n    EvaluateFewShot(\n        eval_fn=matching_net_episode,\n        num_tasks=evaluation_episodes,\n        n_shot=args.n_test,\n        k_way=args.k_test,\n        q_queries=args.q_test,\n        taskloader=evaluation_taskloader,\n        prepare_batch=prepare_nshot_task(args.n_test, args.k_test, args.q_test),\n        fce=args.fce,\n        distance=args.distance\n    ),\n    ModelCheckpoint(\n        filepath=PATH + f'/models/matching_nets/{param_str}.pth',\n        monitor=f'val_{args.n_test}-shot_{args.k_test}-way_acc',\n        # monitor=f'val_loss',\n    ),\n    ReduceLROnPlateau(patience=20, factor=0.5, monitor=f'val_{args.n_test}-shot_{args.k_test}-way_acc'),\n    CSVLogger(PATH + f'/logs/matching_nets/{param_str}.csv'),\n]\n\nfit(\n    model,\n    optimiser,\n    loss_fn,\n    epochs=20,\n    dataloader=background_taskloader,\n    prepare_batch=prepare_nshot_task(args.n_train, args.k_train, args.q_train),\n    callbacks=callbacks,\n    metrics=['categorical_accuracy'],\n    fit_function=matching_net_episode,\n    fit_function_kwargs={'n_shot': args.n_train, 'k_way': args.k_train, 'q_queries': args.q_train, 'train': True,\n                         'fce': args.fce, 'distance': args.distance}\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T12:47:14.660316Z","iopub.execute_input":"2021-10-28T12:47:14.6609Z","iopub.status.idle":"2021-10-28T12:53:27.977107Z","shell.execute_reply.started":"2021-10-28T12:47:14.660859Z","shell.execute_reply":"2021-10-28T12:53:27.975888Z"},"trusted":true},"execution_count":null,"outputs":[]}]}